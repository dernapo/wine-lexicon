---
title: "Analisis del léxico del vino de Comerciales, Expertos y Consumidores"
author: "Javier Sáenz Navajas"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    number_sections: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    code_folding: "hide"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library("data.table")
library("here")
library("dplyr")
library("tidytext")
library("ggplot2")
library("SnowballC")
library("viridis")
library("rmarkdown")
library("stringr")
library("tidyr")

theme_set(theme_bw())
```

# Objetivo

Entender los diferentes léxicos usados por Comerciales, Expertos y Consumidores para sensaciones táctiles y de boca
```{r load_data_chunk}

lexico <- data.table::fread(here::here("data", "lexico.tsv"),
                            select = 1:4,
                            encoding = 'UTF-8')
data.table::setnames(lexico, 
         old = colnames(lexico),
         new = c("tipo", "id", "tactil", "boca"))
```

# Preparar los datos

Un primer vistazo

```{r visualize_chunk, echo=FALSE}

lexico %>%
  paged_table(options = list(rows.print = 5))
```

## Cambiamos el formato de la tabla

De ancho a largo

```{r}
lexico_largo <- data.table::melt(lexico, 
                 id.vars = c("tipo", "id"),
                 variable.name = "sensacion",
                 value.name = "texto")
```

## Pasamos el texto a minúsculas
```{r}
lexico_largo[, texto := tolower(texto)]
lexico_largo[, tipo := tolower(tipo)]


```

## Ajustamos tipos
Para una mejor representación gráfica
```{r}

lexico_largo[, tipo := factor(tipo, levels = c("consumidores", "comercial", "experto"))]
```

## Ajustamos errores gramaticales

```{r}

lexico_largo[, texto:= stringr::str_replace_all(string = texto,
                pattern = "sensacion|sensaciones",
                replacement = "sensación")]

lexico_largo[, texto:= stringr::str_replace_all(string = texto,
                pattern = "sensaciónes",
                replacement = "sensación")]

lexico_largo[, texto:= stringr::str_replace_all(string = texto,
                pattern = "aspera",
                replacement = "aspero")]

```

## Separamos las frases en palabras
Una palabra por fila

```{r}

lexico_largo %>%
  unnest_tokens(output = palabra, input = texto) %>%
  count(tipo, sensacion, palabra, sort = TRUE) -> palabras

```

## Eliminamos "stop words"

Quitamos palabras comunes que no aportan valor, por ejemplo: "y" o "la"

```{r}

stop_words_espanolas <- rbind(data.frame(palabra = c("si", "mas", "nose"), stringsAsFactors = FALSE),
                               data.frame(palabra = tm::stopwords("spanish"), stringsAsFactors = FALSE))


palabras %>% 
  anti_join(stop_words_espanolas, by = "palabra") -> palabras

# guardar datos
fwrite(palabras, here::here("output", "palabras.csv"))

```


## Resultado
```{r}
palabras %>%
  paged_table(options = list(rows.print = 5))
```




# Ánalisis del vocabulario
## Calcular la frequencia de cada palabra

```{r}


palabras_grupos <- palabras %>% 
  group_by(tipo, sensacion) %>% 
  summarize(total = sum(n))

palabras <- left_join(palabras, palabras_grupos, by = c("tipo","sensacion"))

palabras %>%
  paged_table(options = list(rows.print = 5))




```



## Visualizar las palabras más comunes

```{r}
palabras %>% 
  arrange(desc(n)) %>%
  group_by(tipo) %>% 
  top_n(15, n) %>% 
  ungroup() %>%
  mutate(palabra = reorder(palabra, n)) %>%
  ggplot(aes(x= palabra, y = n, fill = tipo)) +
  geom_col(show.legend = FALSE) + 
  facet_grid(sensacion~tipo) +
  coord_flip()  +
  labs(x="", y="Frequencia", 
       title="Palabras más comunes usadas por los consumidores",
       subtitle = "Por tipo de consumdor y sensación",
       caption = "Muestra de 196 consumidores") +
  scale_fill_viridis_d()
```


## Visualizar Frequencia Relativa

```{r frequency_visualization_chunk, error=FALSE, warning=FALSE}

palabras %>%   
    ggplot(aes(n/total, fill = tipo)) +
    geom_histogram(show.legend = FALSE, bins = 30) +
    facet_grid(sensacion~tipo) +
    labs(x="Frequencia relativa",
       y= "Número de veces",
       title = "Número de veces que aparece una palabra con una frequencia relativa dada") +
    scale_fill_viridis_d()

```


## Visualizar tf-idf

Calcular el coeficiente tf_idf nos ayudará a encontrar el vocabulario especíco de cada tipo de consumidor. Que palabras usan unos que los otros casi nunca usan.

```{r}

palabras <- palabras %>% 
  group_by(sensacion) %>%
  group_modify(~ bind_tf_idf(.x, palabra, tipo, n))

palabras %>%
  select(-total) %>%
  arrange(desc(tf_idf)) %>% 
  paged_table(options = list(rows.print = 5))


```

```{r tf_idf_chunck, warning=FALSE}
palabras %>%
  #filter(sensacion=="tactil") %>% 
  arrange(desc(tf_idf)) %>%
  mutate(palabra = factor(palabra, levels = rev(unique(palabra)))) %>% 
  group_by(tipo) %>% 
  top_n(6, wt = tf_idf) %>% 
  #slice(1:5) %>% 
  ungroup() %>%
  mutate(palabra = reorder(palabra, tf_idf)) %>%
  ggplot(aes(palabra, tf_idf, fill = tipo)) +
  geom_col(show.legend = FALSE) +
  labs(x = "", 
       y = "", 
       title = "Léxico específico de sensaciones por tipo de consumidor", 
       subtitle = "Basado en coeficiente tf-idf",
       caption = "") +
  facet_wrap(tipo ~., ncol = 3) +
  coord_flip() +
  scale_fill_viridis_d() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


# Similitudes entre los vocabularios

Usaremos Consine Similarity para calcular los ángulos entre los vectores de palabras usados por cada uno de los tres tipos de usuarios

```{r}
palabras_ancho <- dcast(palabras, 
                        sensacion + palabra ~ tipo, 
                        value.var="n",
                        fill = 0L)

palabras_ancho%>% 
  paged_table(options = list(rows.print = 5))



```

```{r, warning=FALSE}

con_com <- lsa::cosine(palabras_ancho$consumidores, palabras_ancho$comercial)
com_exp <- lsa::cosine(palabras_ancho$comercial, palabras_ancho$experto)
con_exp <- lsa::cosine(palabras_ancho$consumidores, palabras_ancho$experto)


result <- data.table(var1 = c("consumidores", "experto", "experto", "comercial", "experto", "consumidores"),
                     var2 = c("comercial", "consumidores", "comercial", "comercial", "experto","consumidores"),
                     value= c(con_com, con_exp, com_exp, NA, NA, NA))

ggplot(data = result, aes(x=var1, y=var2, fill=value)) + 
  geom_tile() +
  scale_fill_viridis() +
  geom_text(aes(var1, var2, label = round(value, 3)), color = "black", size = 4) +
  theme(legend.position= "none")+
  labs(x="", y="",
       subtitle = "Método: Cosine Similarity sobre vector de palabras usadas",
       title = "El vocabulario de los Expertos es más similar al de los Comerciales \nque al de los consumidores")
  
```

